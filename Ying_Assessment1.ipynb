{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5201: Assessment 1\n",
    "## The Elements of Machine Learning\n",
    "\n",
    "### Objectives\n",
    "This assignment consists of three parts (A,B,C) that assess your understanding of model complexity, model selection, uncertainty in prediction with bootstrapping, and probabilistic machine learning. The total marks of this assessment is 100, and will contribute to the 20% of your final score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A.  Model Complexity and Model Selection\n",
    "In this part, you study the effect of model complexity on the training and testing errors.  You also demonstrate your programming skills by developing a regression algorithm and a cross-validation technique that will be used to select the models with the most effective complexity.\n",
    "\n",
    "__Background__. A KNN regressor is similar to a KNN classifier (covered in Activity 1.1) in that it finds the K nearest neighbors and estimates the value of the given test point based on the values of its neighbours. The main difference between KNN regression and KNN classification is that KNN classifier returns the label that has the majority vote in the neighborhood, whilst KNN regressor returns the average of the neighborsâ€™ values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1-1 KNN regressor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1-1) Implement the KNN regressor function:\n",
    "                                     knn(train.data, train.label, test.data, K=3) \n",
    "which takes the training data and their labels (continuous values), the test set, and the size of the neighborhood (K). It should return the regressed values for the test data points. When choosing the neighbors, you can use the Euclidean distance function to measure the distance between a pair of data points. \n",
    "\n",
    "__Hint__: You are allowed to use KNN classifier code from Activity 1 of Module 1.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "library(corrplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KNN function (distance should be one of euclidean, maximum, manhattan, canberra, binary or minkowski)\n",
    "knn <- function(train.data, train.value, test.data, K=3, distance = 'euclidean'){\n",
    "        ## count number of test samples\n",
    "        train.len <- nrow(train.data)\n",
    "        ## count number of test samples\n",
    "        test.len <- nrow(test.data)\n",
    "\n",
    "        ## calculate distances between samples\n",
    "        dist <- as.matrix(dist(rbind(test.data, train.data), method= distance))[1:test.len, (test.len+1):(test.len+train.len)]\n",
    "\n",
    "        test.predict <- vector(mode=\"numeric\", length=test.len)\n",
    "            ## for each test sample...\n",
    "        for (i in 1:test.len){\n",
    "                ### ...find its K nearest neighbours from training sampels...\n",
    "            nn <- as.data.frame(sort(dist[i,], index.return = TRUE))[1:K,2]\n",
    "                ## nn is the index of training data and also the column index of distance matrix\n",
    "\n",
    "                ###... and calculate the predicted labels according to the majority vote\n",
    "            test.predict[i]<- (mean(train.value[nn]))\n",
    "            }\n",
    "    ## return the class labels as output\n",
    "    return (round(test.predict,2))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q1-2) Plot the training and the testing errors versus 1/K for K=1,..,20 in one plot, using the Task1A_train.csv and Task1A_test.csv datasets provided for this assignment. \n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the training data\n",
    "train.data <- read.csv('./assessments_datasets/Task1A_train.csv')\n",
    "train.value <- train.data[,2]\n",
    "train.data <- train.data[,-2]\n",
    "train.data <- as.data.frame(train.data) # convert train.data from vector to dataframe\n",
    "names(train.data) <- 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the testing data\n",
    "test.data <- read.csv('./assessments_datasets/Task1A_test.csv')\n",
    "test.value <- test.data[,2]\n",
    "test.data <- test.data[,-2]\n",
    "test.data <- as.data.frame(test.data)  # convert test.data from vector to dataframe\n",
    "names(test.data) <- 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe with k rows to store the training and testing errors for each k\n",
    "MSE1 = data.frame('K'= 1:20, 'train' = rep(0,20), 'test' = rep(0,20))\n",
    "# populate the table with training error and testing error\n",
    "# the error is calculated by mean squared error\n",
    "for ( k in 1:20) {\n",
    "    MSE1[k, 'test'] = mean((knn(train.data,train.value,test.data,K=k)-test.value)^2)\n",
    "    MSE1[k, 'train'] = mean((knn(train.data,train.value,train.data,K=k)-train.value)^2)\n",
    "}\n",
    "MSE.m <- melt(MSE1, id = 'K') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(MSE.m) <- c('K','type','error')\n",
    "ggplot(data=MSE.m, aes(x=1/K, y=error, color=type)) + geom_line() +\n",
    "       scale_color_discrete(guide = guide_legend(title = NULL)) + theme_minimal() +\n",
    "       ggtitle(\"Mean Squared Error for training set and testing set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1-3 Report the best value for K in terms of the testing error. Discuss the values of K corresponding to underfitting and overfitting based on your plot in the previous sub-question, Q1-2. \n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value of K should be the one who gives the lowest test error. According to the error plot above, we found that the optimal 1/k lies somewhere near 0.1, with the help of the test error table below, we can identify the optimal k which yields the lowest testing error is 11. \n",
    "\n",
    "From the graph above we can observe two kinds of trend of testing error and training error, the training MSE declines monotonically as 1/k increases whilst the test MSE initially declines before it reaches the optimal point, then it starts to increase again because the model overfits the training model and raise the error caused by variance. Since overfitting happens when the model fits the training set too closely to let randomness and outliers affect the model so that it will perform poorly on testing set. This characteristic can be found in the plot when k ranges from 1 to 12, which shows the training error keeps declining while testing error experience an increase, so it is fair to say from 12 onwards, the model is overfitting. Underfitting happens when the model is simpler than the true model and it fails to capture real trend of data points for both training and testing set. As can be seen in the graph, the MSE of both training set and testing set are declining when 1/k is from 1/20 t0 1/13, this is overfitting as larger k is, the simpler model will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 [K-fold Cross Validation, 20 Marks]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2-1) Implement a K-fold Cross Validation (CV) function for your KNN regressor:  \n",
    "       cv(train.data, train.label, numFold=10) \n",
    "which takes the training data and their labels (continuous values), the number of folds, and returns errors for different folds of the training data. \n",
    "\n",
    "__Hint__: you are allowed to use bootstrap code from Activity 2 of Module 1.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function will give the random index of testing data in each fold\n",
    "getTest <- function (fold = 10, train.length = 42){\n",
    "    indices <- 1: train.length  # get the index of training data\n",
    "    indices <- sample(indices, length(indices))  # shuffle the index\n",
    "    indices.partA <- indices[1:(length(indices)- (length(indices) %% fold))]  # split the index into each folds\n",
    "    size <- length(indices.partA)/fold\n",
    "    test.indices <- split(indices.partA, cumsum( (1:length(indices.partA)-1)%% size ==0) )\n",
    "    if  (length(indices) %% fold != 0 ) {  # assign the remainder index to other folds\n",
    "        for ( i in 1: (length(indices) %% fold)  ) {\n",
    "            test.indices[[i]] <- c(test.indices[[i]], indices[length(indices.partA)+i])\n",
    "        }\n",
    "    }\n",
    "    return (test.indices) # return a list, each element is the index of testing data for a fold\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function will return the matrix of testing error, each row represents the number of nearest neighbour\n",
    "# each column represents the error in each fold\n",
    "cv <- function (train.data, train.value, numFold = 10, knear = 20) {\n",
    "   test.index <- getTest( fold = numFold, train.length = nrow(train.data) )\n",
    "   cvError <- matrix(nrow = knear, ncol = numFold) \n",
    "   for ( k in 1: knear){\n",
    "       for ( f in 1:numFold ){\n",
    "           test.data.chunk <- train.data[test.index[[f]],]  # get the test data in each fold\n",
    "           test.data.chunk <- as.data.frame(test.data.chunk)\n",
    "           test.value.chunk <- train.value[test.index[[f]]] # get the test value in each fold\n",
    "           train.data.chunk <- train.data[-test.index[[f]],] # get the train data in each fold\n",
    "           train.data.chunk <- as.data.frame(train.data.chunk)\n",
    "           train.value.chunk <- train.value[-test.index[[f]]] # get the train data in each fold\n",
    "           \n",
    "           names(train.data.chunk) <- 'X'\n",
    "           names(test.data.chunk) <- 'X'\n",
    "           cvError[k,f] <- mean((knn(train.data.chunk, train.value.chunk, test.data.chunk, K = k) - test.value.chunk)^2)\n",
    "       }\n",
    "   }\n",
    "   return (cvError)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2-2) Using the training data, run your K-fold CV where the numFold is set to 10. Change the value of K=1,..,20 and for each K compute the average 10 error numbers you have got.  Plot the average error numbers versus 1/K for K=1,..,20. Further, add two dashed lines around the average error indicating the average +/- standard deviation of errors. Include the plot in your report.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# aggregate the matrix to generate the mean and standard deviaion of error\n",
    "kfold.result <- matrix(nrow = 20, ncol = 4)\n",
    "cvError <- cv(train.data, train.value)\n",
    "for (k in 1:20){\n",
    "    kfold.result[k,1] <- k\n",
    "    kfold.result[k,2] <- mean(cvError[k,])\n",
    "    kfold.result[k,3] <- mean(cvError[k,]) + sd(cvError[k,])\n",
    "    kfold.result[k,4] <-  mean(cvError[k,]) - sd(cvError[k,])\n",
    "}\n",
    "kfold.result <- as.data.frame(kfold.result)\n",
    "names(kfold.result) <- c(\"k\",\"MSE\",\"plus_sd\",\"minus_sd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt the matrix \n",
    "kfold.result.m <- melt(kfold.result, id = \"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the MSE and their range within 1 standard deviation, color and line type are used to differentiate the lines\n",
    "ggplot(data=kfold.result.m, aes(x=1/k, y= value, color=variable)) + geom_line(aes(linetype=variable, color=variable)) +\n",
    "       scale_color_discrete(guide = guide_legend(title = NULL)) + theme_minimal() +\n",
    "       ggtitle(\"K-fold cross validation\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2-3) Report the values of K that result to minimum average error and minimum standard deviation of errors based on your cross validation plot in the previous sub-question, Q2-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Based on the graph above and the table below, we can see that when 1/k equals 0.5, i.e. k is 2, the mean squared error reached the lowest point and the distance between 1 sd above and below is most narrow, which indicates that 2 is the optimal value for k that can gives the lowest error and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B. Prediction Uncertainty with Bootstrapping\n",
    "This part is the adaptation of Activity 2 from KNN classification to KNN regression. You use the bootstrapping technique to quantify the uncertainty of predictions for the KNN regressor that you implemented in Part A. \n",
    "\n",
    "#### Question 3 [Bootstrapping, 20 Marks]\n",
    "Q3-1) Modify the code in Activity 2 to handle bootstrapping for KNN regression. \n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return the index for bootstrapping training set \n",
    "boot <- function (original.size=930, sample.size=25, times=100){\n",
    "    indx <- matrix(nrow=times, ncol=sample.size) # create an empty matrix\n",
    "    for (t in 1:times){\n",
    "        indx[t, ] <- sample(x=original.size, size=sample.size, replace = TRUE)\n",
    "    }\n",
    "    return(indx)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3-2)Load Task1B_train.csv and Task1B_test.csv sets. Apply your bootstrapping for KNN regression with times = 100 (the number of subsets), size = 25 (the size of each subset), and change K=1,..,20 (the neighbourhood size). Now create a boxplot where the x-axis is K, and the y-axis is the average error (and the uncertainty around it) corresponding to each K.  \n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the training and testing data\n",
    "trainB <- read.csv('./assessments_datasets/Task1B_train.csv')\n",
    "testB <- read.csv('./assessments_datasets/Task1B_test.csv')\n",
    "train.dataB <- trainB[,-5] # extract independent values of training data\n",
    "train.valueB <- trainB[,5] # extract dependent values of training data\n",
    "test.dataB <- testB[,-5] # extract independent values of testing data\n",
    "test.valueB <- testB[,5] # extract dependent values of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boot.indx <- boot(nrow(train.dataB),25,100 )  # get the training index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 20\n",
    "L = 100\n",
    "\n",
    "# a dataframe to track the number of missclassified samples in each case\n",
    "MSE <- data.frame('K'=1:K, 'L'=1:L, 'test'=rep(0,L*K))\n",
    "\n",
    "# THIS MAY TAKE A FEW MINUTES TO COMPLETE\n",
    "## for every k values:\n",
    "for (k in 1: K){\n",
    "    \n",
    "    ### for every dataset sizes:\n",
    "    for (l in 1:L){\n",
    "        \n",
    "        #### calculate iteration index i\n",
    "        i <- (k-1)*L+l\n",
    "        \n",
    "        #### save sample indices that were selected by bootstrap\n",
    "        indx <- boot.indx[l,]\n",
    "        \n",
    "        #### save the value of k and l\n",
    "        MSE[i,'K'] <- k\n",
    "        MSE[i,'L'] <- l\n",
    "        \n",
    "        #### calculate and record the train and test missclassification rates\n",
    "        MSE[i,'test'] <-  mean((knn(train.dataB[indx,], train.valueB[indx], test.dataB, k)-test.valueB)^2)\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot the boostrapping result for different k\n",
    "ggplot(data=MSE, aes(factor(K), test, fill = 'red') )+ geom_boxplot(outlier.shape = NA)  + \n",
    "    scale_color_discrete(guide = guide_legend(title = NULL)) + \n",
    "    ggtitle('MSE vs. K (Box Plot)') + theme_minimal()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3-3) Based on your plot in the previous sub-question, Q3-2, how does the test error and its uncertainty behave as K increases? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "As k increases, the test error becomes larger while the uncertainty initially increases significantly and then become stable at a certain level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3-4) Load Task1B_train.csv and Task1B_test.csv sets. Apply your bootstrapping for KNN regression with K=10 (the neighbourhood size), size = 25 (the size of each subset), and change times = 10, 20, 30,.., 200 (the number of subsets). Now create a boxplot where the x-axis is â€˜timesâ€™, and the y-axis is the average error (and the uncertainty around it) corresponding to each value of â€˜timesâ€™. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times <- seq(10,200,10) # get a vector of times\n",
    "cumTimes <- cumsum(times)  # get a vector of cumulative value of times\n",
    "total.size <- nrow(train.dataB)\n",
    "btError <- matrix(nrow = sum(times), ncol = 3) # get a matrix to store value of MSE for each time \n",
    "btError <- as.data.frame(btError)\n",
    "names(btError) <- c('T','indx','MSE')\n",
    "for (t in times) {\n",
    "    indx <- match(t, times) # get the position of t in times vector\n",
    "    boot.index <- boot(total.size, 25, t)\n",
    "    btError[(cumTimes[indx]-t+1):cumTimes[indx],'T'] <- t  # populate the first column with the value of times \n",
    "    btError[(cumTimes[indx]-t+1):cumTimes[indx],'indx'] <- 1:t  # populate the second column with the value of subset index\n",
    "    for (i in 1:t) {\n",
    "        indxA <- boot.index[i,]\n",
    "        # populate the third column with the MSE of the subset \n",
    "        btError[cumTimes[indx]-t+i,'MSE']  <- (mean((knn(train.dataB[indxA,], train.valueB[indxA], test.dataB, K = 10)-test.valueB)^2))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot the boostrapping result for different times\n",
    "ggplot(data=btError, aes(factor(T), MSE, fill = 'red') )+ geom_boxplot(outlier.shape = NA)  + \n",
    "    scale_color_discrete(guide = guide_legend(title = NULL)) + \n",
    "    ggtitle('MSE vs. K (Box Plot)') + theme_minimal()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3-5) Based on your plot in the previous sub-question, Q3-4, how does the test error and its uncertainty behave as the number of subsets in bootstrapping increases? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "As the number of dataset increases, the errors tend to be stable and not deviate too much from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C. Probabilistic Machine Learning\n",
    "In this part, you show your knowledge about the foundation of the probabilistic machine learning (i.e. probabilistic inference and modeling) by solving two simple but basic statistical inference problems. Solve the following problems based on the probability concepts you have learned in Module 1 with the same math conventions. Please show your work in your report. Also, there are two conceptual questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Question 4 [Bayes Rule, 20 Marks] \n",
    "Recall the simple example from Appendix A of Module 1. Suppose we have one red and one blue box. In the red box we have 2 apples and 6 oranges, whilst in the blue box we have 3 apples and 1 orange. Now suppose we randomly selected one of the boxes and picked a fruit. If the picked fruit is an apple, what is the probability that it was picked from the blue box?\n",
    "\n",
    "Note that the chance of picking the red box is 40% and the selection chance for any of the pieces from a box is equal for all the pieces in that box.\n",
    "\n",
    "**Answer**\n",
    "<img src=\"Q4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 5 [Maximum Likelihood, 20 Marks] \n",
    "As opposed to a coin which has two faces, a dice has 6 faces. Suppose we are given a dataset which contains the outcomes of 10 independent tosses of a dice: D:={1,4,5,3,1,2,6,5,6,6}. We are asked to build a model for this dice, i.e. a model which tells what is the probability of each face of the dice if we toss it. Using the maximum likelihood principle, please determine the best value for our model parameters.\n",
    "\n",
    "Hint: You can use a multinomial distribution with 6 probability parameters, each of which corresponding to a dice face (as opposed to coin where there are two parameters). You need to form the likelihood objective function, and then maximise it by setting the derivative with respect to the parameters to zero. Since the probabilities must sum up to 1, you only need to maximise the likelihood objective with respect to five parameters; the value of the sixth parameter is then going to be one minus the sum of the other parameters.\n",
    "\n",
    "**Answer**\n",
    "<img src = \"Q5-1.png\">\n",
    "<img src = \"Q5-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: \n",
    "As you have seen through the module, you are generally in the position of choosing a less flexible or more flexible model for your regression or classification problem. You need to be aware that we choose a model to serve to our final goal regardless of the flexibility level. It means that we may prefer a less flexible model to a more flexible model, or vice versa. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The advantages of flexibale model is that it will fit well on the trainning data and yields a very low training error. As the level of flexibility increases, the model will fit the observed training data more closely. Because the flexible will capture the slight variations in the data, it will result in very low bias. The disadvantage of flexible models is that they tend to cause high variance of test error because changing any\n",
    "one of the data points may cause estimates to change considerably. The flexible model works too hard to find patterns in the training data and\n",
    "may be picking up some patterns that are just caused by random chance\n",
    "rather than by true properties of the unknown true model, which is known as over-fitting.\n",
    "\n",
    "\n",
    "when the true decision boundary is complex and the true model of data has high level of freedom, a flexible method is preferred. If we come from a variance-bias point of view, when the error is dominated by bias rather than variance, a more flexible model will contribute more to the decrease of error. However when variance has more impact on the error than bias, i.e. as the flexibility rise up, the variance will increase rapidly, in this case a more restricted model will outperform the high flexible model.\n",
    "#### Question 7: \n",
    "Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non- parametric approach)? What are its disadvantages?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "A parametric approach reduces the problem of estimating f down to one of estimating a set of parameters because it assumes a form for f.\n",
    "\n",
    "A non-parametric approach does not assume a patricular form of f and so requires a very large sample to accurately estimate f.\n",
    "\n",
    "The advantages of a parametric approach to regression or classification are the simplifying of modeling ff to a few parameters and not as many observations are required compared to a non-parametric approach.\n",
    "\n",
    "The disadvantages of a parametric approach to regression or classification are a potentially inaccurate estimate f if the form of f assumed is wrong or to overfit the observations if more flexible models are used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
